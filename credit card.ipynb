{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-frawd rate is 0.9980166666666667\n",
      "col_0      0   1\n",
      "Class           \n",
      "0      59876   5\n",
      "1         35  84\n",
      "Overall accuracy:99.93\n",
      "False Positive Error rate: 29.41%\n",
      "False Negative Error rate: 5.62%\n",
      "True Poitive rate: 70.59%\n",
      "True Negative rate: 99.99%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "rawdata=pd.read_csv(r'C:\\Users\\jisaa\\Desktop\\creditcard.csv\\creditcard.csv')\n",
    "rawdata=data.drop(data.columns[0], axis=1)\n",
    "datasliced0=rawdata.iloc[:,:-1]\n",
    "datasliced1=rawdata.iloc[:,-1:]\n",
    "names0=datasliced0.columns\n",
    "data= pd.DataFrame(preprocessing.scale(datasliced0), columns=names0)\n",
    "data['Class']=datasliced1\n",
    "\n",
    "i=1\n",
    "while i<len(data.columns)-1:\n",
    "    array=data.iloc[:,i]\n",
    "    amean=array.mean()\n",
    "    astd=array.std()\n",
    "    array=[x if x>=-(4*astd) else -4*astd for x in array]\n",
    "    array=[x if x<=4*astd else 4*astd for x in array]\n",
    "    array=np.asarray(array)\n",
    "    data.iloc[:,i]=array\n",
    "    i+=1\n",
    "\n",
    "\n",
    "\n",
    "X=data.loc[:, data.columns != 'Class']\n",
    "y=data['Class']\n",
    "Xcolumns=X.columns.values\n",
    "ycolumns=['Class']\n",
    "os=SMOTE(random_state=0)\n",
    "os_X,os_y=os.fit_resample(X,y)\n",
    "X = pd.DataFrame(data=os_X,columns=Xcolumns )\n",
    "y= pd.DataFrame(data=os_y,columns=ycolumns)\n",
    "\n",
    "test=data.iloc[-60000:]\n",
    "data=data.iloc[:-60000]\n",
    "X=data.loc[:, data.columns != 'Class']\n",
    "y=data['Class']\n",
    "testX=test.loc[:, test.columns != 'Class']\n",
    "testy=test['Class']\n",
    "\n",
    "result=logreg.fit(X,y) \n",
    "\n",
    "pred= result.predict(testX)\n",
    "pred_y = np.where(pred < .5, 0, 1)\n",
    "table = pd.crosstab(testy, pred_y)\n",
    "\n",
    "frawdrate=len(test.loc[lambda x: x['Class']==0,])/test.shape[0]\n",
    "\n",
    "print(\"Non-frawd rate is {}\".format(frawdrate))\n",
    "print(table)\n",
    "print(\"Overall accuracy:{:.2f}\".format((table.iloc[0,0] + table.iloc[1,1]) / (table.sum().sum())*100))\n",
    "print(\"False Positive Error rate: {:.2f}%\".format((table.iloc[1,0])/(table.iloc[1,0] + table.iloc[1,1])*100))\n",
    "print(\"False Negative Error rate: {:.2f}%\".format((table.iloc[0,1])/(table.iloc[0,1] + table.iloc[1,1])*100))\n",
    "print(\"True Poitive rate: {:.2f}%\".format((table.iloc[1,1])/(table.iloc[1,0] + table.iloc[1,1])*100))\n",
    "print(\"True Negative rate: {:.2f}%\".format((table.iloc[0,0])/(table.iloc[0,0] + table.iloc[0,1])*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retired Code\n",
    "\n",
    "### Imports\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib qt\n",
    "\n",
    "### Manual Oversampling\n",
    "    data0=data.loc[lambda x: x['Class']==0,]\n",
    "    data1=data.loc[lambda x: x['Class']==1,]\n",
    "    nrows=data0.shape[0]-data1.shape[0]\n",
    "    drop_indices=np.random.choice(data0.index,nrows, replace=False)\n",
    "    data0= data0.drop(drop_indices)\n",
    "    data=pd.concat([data0,data1])\n",
    "\n",
    "### Used Scipy.stats.shapiro to measure how variables were normally distributed\n",
    "    print(\"Before transforming Outliers\")\n",
    "    i=1\n",
    "    while i<len(data.columns)-1:\n",
    "        x=data.iloc[0:4999,i-1:i].values\n",
    "        ttest=stats.shapiro(x)[0]\n",
    "        print(\"v{}: Shapiro {}\".format(i,ttest))\n",
    "        i+=1\n",
    "        print(\"After transforming Outliers\")\n",
    "        i=1\n",
    "    while i<len(data.columns)-1:\n",
    "        x=data.iloc[0:4999,i-1:i].values\n",
    "        ttest=stats.shapiro(x)[0]\n",
    "        print(\"v{}: Shapiro {}\".format(i,ttest))\n",
    "        i+=1  \n",
    "    \n",
    "### Graphed Variable population against normal curve to visually assess normal distribution\n",
    "    plt.hist(array,color='red',normed=True)\n",
    "    mu, std=stats.norm.fit(array)\n",
    "    x=np.linspace(array.min(),array.max(),100)\n",
    "    ndf=stats.norm.pdf(x, mu, std)\n",
    "    plt.plot(x, ndf,'k', label=\"Norm\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "### I was initially worried about overfitting, so I reduced the variables using recursive feature elimination\n",
    "    logreg = LogisticRegression()\n",
    "    rfe = RFE(logreg, 29)\n",
    "    rfe = rfe.fit(X,y.values.ravel())\n",
    "    selection=rfe.support_\n",
    "\n",
    "    selectedvar=list()\n",
    "    for i,sel in zip(X,selection):\n",
    "        if sel:\n",
    "            selectedvar.append(i)\n",
    "    selectedvar.append(\"Class\")\n",
    "    data=data[selectedvar]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
